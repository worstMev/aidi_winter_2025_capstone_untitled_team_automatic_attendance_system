{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import cv2\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "import io\n",
    "import time\n",
    "from mss import mss\n",
    "from datetime import date\n",
    "\n",
    "## Create the client connector to S3 \n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id='AKIATG6MGZ27MEZI7EM3',\n",
    "    aws_secret_access_key='NjwfpnIGoKqZYNneFLd6MxRc9QMc15w8YXi5jD3a',\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "# Create a session with your AWS access keys and region\n",
    "session = boto3.session.Session(\n",
    "    aws_access_key_id='AKIATG6MGZ27MEZI7EM3',\n",
    "    aws_secret_access_key='NjwfpnIGoKqZYNneFLd6MxRc9QMc15w8YXi5jD3a',\n",
    "    region_name='us-east-2'  # Change to your desired region\n",
    ")\n",
    "\n",
    "# Create a client for Secrets Manager (or any other service)\n",
    "client = session.client('secretsmanager')\n",
    "\n",
    "\n",
    "bucket_n = \"face-rec-database\"\n",
    "\n",
    "prefix = \"test_data_set/\"\n",
    "\n",
    "\n",
    "response = s3.list_objects_v2(Bucket=bucket_n, Prefix=prefix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "import snowflake.connector as sc\n",
    "\n",
    "\n",
    "## function to get the snowflake private key from aws secret manager\n",
    "def get_secret():\n",
    "    secret_name = \"snowflake/face_rec/rsa_private_key\"\n",
    "    region_name = \"us-east-2\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "\n",
    "    secret = get_secret_value_response['SecretString']\n",
    "    return secret\n",
    "\n",
    "\n",
    "\n",
    "## funct to convert the key to appropiate format accepte by snowflake connector \n",
    "def load_private_key(pem_str, key_password=None):\n",
    "    \"\"\"\n",
    "    Load a PKCS8 formatted private key from a PEM string.\n",
    "    key_password should be a string if the key is encrypted, otherwise None.\n",
    "    \"\"\"\n",
    "    private_key = serialization.load_pem_private_key(\n",
    "        pem_str.encode(\"utf-8\"),\n",
    "        password=key_password.encode(\"utf-8\") if key_password else None,\n",
    "        backend=default_backend()\n",
    "    )\n",
    "    return private_key\n",
    "\n",
    "# Retrieve the private key string from AWS Secrets Manager.\n",
    "pem_private_key = get_secret()\n",
    "\n",
    "# If your key is not encrypted, key_password is None.\n",
    "key_password = None  # Set this if your private key is password-protected\n",
    "\n",
    "# Load the private key into a cryptography object.\n",
    "private_key = load_private_key(pem_private_key, key_password=key_password)\n",
    "\n",
    "# Snowflake connection parameters using key-pair authentication.\n",
    "conn_params = {\n",
    "    'account': 'TIMCEXC-MYB81917',\n",
    "    'user': 'face_rec_service',\n",
    "    'private_key': private_key,\n",
    "    'warehouse': 'COMPUTE_WH',\n",
    "    'database': 'RECOG_DB',\n",
    "    'schema': 'ATTEND'\n",
    "}\n",
    "\n",
    "# Connect to Snowflake.\n",
    "ctx = sc.connect(**conn_params)\n",
    "cs = ctx.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query.\n",
    "cs.execute(\"SELECT *  FROM RECOG_DB.ATTEND.institution\")\n",
    "print(cs.fetchone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = []\n",
    "\n",
    "# List objects in the S3 bucket with the specified prefix\n",
    "response = s3.list_objects_v2(Bucket=bucket_n, Prefix=prefix)\n",
    "\n",
    "for obj in response.get(\"Contents\", []):\n",
    "    key = obj[\"Key\"]\n",
    "    if not key.endswith((\".jpg\", \".jpeg\", \".png\",\"JPG\")):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Retrieve the image from S3\n",
    "        s3_response = s3.get_object(Bucket=bucket_n, Key=key)\n",
    "        img_bytes = s3_response[\"Body\"].read()\n",
    "\n",
    "        # Convert the byte stream to a NumPy array and then decode to an image\n",
    "        np_arr = np.frombuffer(img_bytes, np.uint8)\n",
    "        img = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n",
    "\n",
    "        # Pass the image directly to DeepFace\n",
    "        objs = DeepFace.represent(img_path=img, model_name=\"Facenet\", detector_backend= 'retinaface')\n",
    "        embedding = objs[0][\"embedding\"]\n",
    "        instances.append((key, embedding))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {key}: {e}\")\n",
    "\n",
    "# Now, instances contains tuples of (s3_object_key, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the number of images embbeded 6\n"
     ]
    }
   ],
   "source": [
    "## incremental de instances \n",
    "\n",
    "\n",
    "preloaded_imgs =  cs.execute (\"\"\"select  img_name\n",
    "from RECOG_DB.ATTEND.embeddings_test\n",
    "order by ID\"\"\").fetchall()\n",
    "\n",
    "\n",
    "### this list contains all images that are already on the embbedings \n",
    "preloaded_img_list  = [row[0] for row in preloaded_imgs ]\n",
    "\n",
    "\n",
    "##print(f\"This is the embedings list before. \\n {preloaded_img_list}\")\n",
    "\n",
    "\n",
    "## initialize an empty list that will containg all new images embeddings \n",
    "instances = []\n",
    "\n",
    "# List objects in the S3 bucket with the specified prefix\n",
    "response = s3.list_objects_v2(Bucket=bucket_n, Prefix=prefix)\n",
    "count = 0 \n",
    "\n",
    "## for loop that will create embeding for all new images in the s3 bucket\n",
    "for obj in response.get(\"Contents\", []):\n",
    "    key = obj[\"Key\"]\n",
    "\n",
    "    ## if image title end in a img format it will continue \n",
    "    if not key.endswith((\".jpg\", \".jpeg\", \".png\",\"JPG\")):\n",
    "        continue\n",
    "    ## if image title is already contained in the embbegings table itll be ignored\n",
    "    elif key in preloaded_img_list:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Retrieve the image from S3\n",
    "        s3_response = s3.get_object(Bucket=bucket_n, Key=key)\n",
    "        img_bytes = s3_response[\"Body\"].read()\n",
    "\n",
    "        # Convert the byte stream to a NumPy array and then decode to an image\n",
    "        np_arr = np.frombuffer(img_bytes, np.uint8)\n",
    "        img = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n",
    "\n",
    "        # Pass the image directly to DeepFace\n",
    "        \n",
    "        objs = DeepFace.represent(img_path=img, \n",
    "                                  model_name=\"Facenet\", \n",
    "                                  detector_backend= 'mtcnn',  \n",
    "                                  anti_spoofing = False)\n",
    "        embedding = objs[0][\"embedding\"]\n",
    "        instances.append((key, embedding))\n",
    "        count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {key}: {e}\")\n",
    "\n",
    "\n",
    "print(f\"this is the number of images embbeded {count}\")\n",
    "\n",
    "\n",
    "## insert the new embeddings in the snowflake table \n",
    "\n",
    "for  img_path, embedding in instances:\n",
    "    # Join embedding values into a comma-separated string\n",
    "    vec = str(embedding)\n",
    "    statement = f\"\"\"\n",
    "        INSERT INTO RECOG_DB.ATTEND.embeddings_test (img_name, embedding)\n",
    "            SELECT '{img_path}', {vec}::VECTOR(FLOAT,128)\n",
    "        \"\"\"\n",
    "    cs.execute(statement)\n",
    "\n",
    "\n",
    "##preloaded_imgs_after =  cs.execute (\"\"\"select  img_name\n",
    "##from RECOG_DB.ATTEND.embeddings_test\n",
    "##order by ID\"\"\").fetchall()\n",
    "##\n",
    "##\n",
    "##preloaded_img_list_after  = [row[0] for row in preloaded_imgs_after ]\n",
    "##\n",
    "##print(f\"This is the embedings list after. \\n {preloaded_img_list_after}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE TO STORE EMBBEDINGS FROM A EMBEDGINGS LIST INTO SNOWFLAKE\n",
    "\n",
    "\n",
    "###for  img_path, embedding in instances:\n",
    "###    # Join embedding values into a comma-separated string\n",
    "###    vec = str(embedding)\n",
    "###    statement = f\"\"\"\n",
    "###        INSERT INTO RECOG_DB.ATTEND.embeddings_test (img_name, embedding)\n",
    "###            SELECT '{img_path}', {vec}::VECTOR(FLOAT,128)\n",
    "###        \"\"\"\n",
    "###    cs.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### insert sintetic data into the refference databse \n",
    "## in order to check the speed on the search on larger data sets. \n",
    "\n",
    "syn_data = []\n",
    "target_size = 100000\n",
    "for i in range(target_size):\n",
    "    img_name = f\"synthetic_{i}.jpg\"\n",
    "    # generate 128-d embedding with dimension values in [-5, +5]\n",
    "    embedding = np.random.uniform(-5, 5, 128).tolist()\n",
    "    syn_data.append((img_name, embedding))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### insert sintetic data into the refference databse \n",
    "## in order to check the speed on the search on larger data sets.\n",
    "for  img_path, embedding in syn_data:\n",
    "    # Join embedding values into a comma-separated string\n",
    "    vec = str(embedding)\n",
    "    statement = f\"\"\"\n",
    "        INSERT INTO RECOG_DB.ATTEND.embeddings_test (img_name, embedding)\n",
    "            SELECT '{img_path}', {vec}::VECTOR(FLOAT,128)\n",
    "        \"\"\"\n",
    "    cs.execute(statement)\n",
    "\n",
    "\n",
    "### DONT USE --> TOO SLOW ^\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "### testing with one image on a local directory \n",
    "\n",
    "test_img =  r'C:\\Users\\User\\Pictures\\IMG_3962.png' \n",
    "\n",
    "test_objs = DeepFace.represent(img_path=test_img, model_name=\"Facenet\", anti_spoofing= False)\n",
    "test_embedding = str(test_objs[0][\"embedding\"])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_objs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##comparing the test image with the data base\n",
    "\n",
    "result = cs.execute(f\"\"\"\n",
    "    select  IMG_NAME, VECTOR_L2_DISTANCE(EMBEDDING,{test_embedding}::VECTOR(FLOAT,128)) as SIMILARITY\n",
    "    from RECOG_DB.ATTEND.embeddings_test\n",
    "    ORDER BY SIMILARITY ASC\n",
    "    limit 1\n",
    "\"\"\")\n",
    "\n",
    "bestmacth = result.fetchall()\n",
    "\n",
    "print(bestmacth)\n",
    "\n",
    "str(bestmacth[0][0]).split('/')[-1].split('.')[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition error: Spoof detected in the given image.\n",
      "Recognition error: Spoof detected in the given image.\n",
      "Recognition error: Face could not be detected in numpy array.Please confirm that the picture is a face photo or consider to set enforce_detection param to False.\n"
     ]
    }
   ],
   "source": [
    "### testing with an vedeo stream from my second monitor\n",
    "\n",
    "\n",
    "\n",
    "# Load Haar cascade for face detection.\n",
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "\n",
    "# Optionally, precompute your reference embeddings.\n",
    "# For example, you might have a folder \"reference_faces\" with subfolders or images named with the label.\n",
    "# You can run this once at startup to build your embeddings database.\n",
    "# Alternatively, DeepFace.find() can search a folder directly.\n",
    "\n",
    "\n",
    "##db_path = r\"C:\\Users\\User\\Pictures\\Face_recog_db\"\n",
    "\n",
    "def detect_and_recognize(screen_frame):\n",
    "    # Convert frame to grayscale for face detection.\n",
    "    gray_image = cv2.cvtColor(screen_frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract face region of interest (ROI)\n",
    "        face_roi = screen_frame[y:y+h, x:x+w]\n",
    "        # Convert ROI from BGR to RGB (DeepFace expects RGB images)\n",
    "        face_roi_rgb = face_roi ##cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        try:\n",
    "            # Use DeepFace.find() to compare the detected face against your reference database.\n",
    "            # Setting enforce_detection=False to avoid errors if a face isn't perfectly aligned.\n",
    "            \n",
    "            \n",
    "            ##results = DeepFace.find(img_path=face_roi_rgb, db_path=db_path, model_name= \"Facenet\", enforce_detection=False)\n",
    "            results = DeepFace.represent(img_path=face_roi_rgb, model_name=\"Facenet\", anti_spoofing= True)\n",
    "\n",
    "\n",
    "            if len(results) >0 and len(results[0][\"embedding\"]) > 0:\n",
    "\n",
    "                embedding_target = results[0][\"embedding\"] \n",
    "\n",
    "                snow_query_search = f\"\"\"\n",
    "                                        SELECT   IMG_NAME, VECTOR_L2_DISTANCE(EMBEDDING,{embedding_target}::VECTOR(FLOAT,128)) as SIMILARITY\n",
    "                                        FROM RECOG_DB.ATTEND.embeddings_test\n",
    "                                        ORDER BY SIMILARITY ASC\n",
    "                                        LIMIT 1\n",
    "                                    \"\"\"\n",
    "                \n",
    "                \"\"\"insert into RECOG_DB.ATTEND.attendance \n",
    "                \"\"\"\n",
    "                \n",
    "                best_match = cs.execute(snow_query_search).fetchall()\n",
    "\n",
    "                label = best_match[0][0].split('/')[-1].split('.')[0]\n",
    "                \n",
    "            else:\n",
    "                label = 'unknown'\n",
    "                \n",
    "            ##if len(results) > 0 and len(results[0]) > 0:\n",
    "            ##    # Assuming results is a list of DataFrames (one per model)\n",
    "            ##    # Get the first match's identity, then extract the name from the file path.\n",
    "            ##    identity_path = results[0].iloc[0][\"identity\"]\n",
    "            ##    label = identity_path.split(\"/\")[-1].split(\"\\\\\")[-1]  # works for Linux/Mac and Windows paths\n",
    "            ##else:\n",
    "            ##    label = \"Unknown\"\n",
    "        except Exception as e:\n",
    "            # If DeepFace fails to process, label as Unknown.\n",
    "            label = \"Unknown_from_exception\"\n",
    "            print(\"Recognition error:\", e)\n",
    "        \n",
    "        # Draw rectangle and label on the frame.\n",
    "        cv2.rectangle(screen_frame, (x, y), (x+w, y+h), (0, 255, 0), 4)\n",
    "        cv2.putText(screen_frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    return screen_frame\n",
    "\n",
    "# Capture the screen feed from the second monitor.\n",
    "with mss() as sct:\n",
    "    monitor = sct.monitors[2]\n",
    "    last_sent = 0  # time when the last frame was processed/sent\n",
    "    while True:\n",
    "        # Grab the screen image.\n",
    "        img = np.array(sct.grab(monitor))\n",
    "        # Convert image from BGRA to BGR.\n",
    "        screen_frame = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "        \n",
    "        # Check if 5 seconds have passed since the last processed frame\n",
    "        now = time.time()\n",
    "        if now - last_sent >= 20:\n",
    "            processed_frame = detect_and_recognize(screen_frame)\n",
    "            cv2.imshow(\"My Face Detection & Recognition Project\", processed_frame)\n",
    "            last_sent = now  # update last_sent time\n",
    "\n",
    "        # Even if not sending a frame, check for the 'q' key press to exit.\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_recognize(screen_frame):\n",
    "    # Convert frame to grayscale for face detection.\n",
    "    gray_image = cv2.cvtColor(screen_frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))\n",
    "\n",
    "    datest =date.today()\n",
    "    formatted_date = datest.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract face region of interest (ROI)\n",
    "        face_roi = screen_frame[y:y+h, x:x+w]\n",
    "        # Convert ROI from BGR to RGB (DeepFace expects RGB images)\n",
    "        face_roi_rgb = face_roi ##cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        try:\n",
    "            # Use DeepFace.find() to compare the detected face against your reference database.\n",
    "            # Setting enforce_detection=False to avoid errors if a face isn't perfectly aligned.\n",
    "            \n",
    "            \n",
    "            ##results = DeepFace.find(img_path=face_roi_rgb, db_path=db_path, model_name= \"Facenet\", enforce_detection=False)\n",
    "            results = DeepFace.represent(img_path=face_roi_rgb, model_name=\"Facenet\", detector_backend= 'opencv')\n",
    "\n",
    "\n",
    "            if len(results) >0 and len(results[0][\"embedding\"]) > 0:\n",
    "\n",
    "                embedding_target = results[0][\"embedding\"] \n",
    "\n",
    "                snow_query_search = f\"\"\"\n",
    "                                        SELECT   IMG_NAME, VECTOR_L2_DISTANCE(EMBEDDING,{embedding_target}::VECTOR(FLOAT,128)) as SIMILARITY\n",
    "                                        FROM RECOG_DB.ATTEND.embeddings_test\n",
    "                                        ORDER BY SIMILARITY ASC\n",
    "                                        LIMIT 1\n",
    "                                    \"\"\"\n",
    "                \n",
    "                \n",
    "                best_match = cs.execute(snow_query_search).fetchall()\n",
    "\n",
    "                label = best_match[0][0].split('/')[-1].split('.')[0]\n",
    "\n",
    "                attendance_merge = f\"\"\"\n",
    "                MERGE INTO RECOG_DB.ATTEND.attendance_test AS target\n",
    "                USING (\n",
    "                    SELECT '{label}' AS student_name,\n",
    "                           'CAPSTONE_PROJECT' AS class_name,\n",
    "                           '{formatted_date}'::DATE AS attendance_date\n",
    "                ) AS source\n",
    "                ON target.student_name = source.student_name \n",
    "                   AND target.attendance_date = source.attendance_date\n",
    "                WHEN NOT MATCHED THEN\n",
    "                    INSERT (student_name, class_name, attendance_date)\n",
    "                    VALUES (source.student_name, source.class_name, source.attendance_date)\n",
    "                \"\"\"\n",
    "                cs.execute(attendance_merge)\n",
    "\n",
    "                print(f'detection success: {label} detected')\n",
    "                \n",
    "            else:\n",
    "                label = 'unknown'\n",
    "                \n",
    "\n",
    "        except Exception as e:\n",
    "            # If DeepFace fails to process, label as Unknown.\n",
    "            label = \"Unknown_from_exception\"\n",
    "            print(\"Recognition error:\", e)\n",
    "        \n",
    "        # Draw rectangle and label on the frame.\n",
    "        cv2.rectangle(screen_frame, (x, y), (x+w, y+h), (0, 255, 0), 4)\n",
    "        cv2.putText(screen_frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    return screen_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detection success: Juan_esguerra detected\n"
     ]
    }
   ],
   "source": [
    "##path = r'C:\\Users\\User\\Pictures\\juan_esguerra.png'\n",
    "\n",
    "path = r'C:\\Users\\User\\Pictures\\Face_recog_db\\Juan_esguerra.JPG'\n",
    "\n",
    "\n",
    "img =cv2.imread(path)\n",
    "img = np.array(img)\n",
    "        # Convert image from BGRA to BGR.\n",
    "screen= cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "processed_frame = detect_and_recognize(screen)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def repre(path):\n",
    "        try :\n",
    "        \n",
    "            results = DeepFace.represent(img_path= path , model_name=\"Facenet\", detector_backend= 'opencv')\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "                    # If DeepFace fails to process, label as Unknown.\n",
    "                    label = \"Unknown_from_exception\"\n",
    "                    print(\"Recognition error:\", e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'embedding': [-0.43879178166389465,\n",
       "   -0.1992749720811844,\n",
       "   -0.8512336015701294,\n",
       "   3.4066812992095947,\n",
       "   0.9236522316932678,\n",
       "   -2.223020076751709,\n",
       "   0.7528871297836304,\n",
       "   -0.8143245577812195,\n",
       "   -1.3066959381103516,\n",
       "   1.5445231199264526,\n",
       "   -0.1213134154677391,\n",
       "   -0.765791654586792,\n",
       "   2.660133123397827,\n",
       "   -0.9077204465866089,\n",
       "   -0.7419415712356567,\n",
       "   -0.014896124601364136,\n",
       "   0.3214260935783386,\n",
       "   -0.9788164496421814,\n",
       "   -1.7662876844406128,\n",
       "   -1.1483627557754517,\n",
       "   -1.8599640130996704,\n",
       "   1.6605465412139893,\n",
       "   -1.0344752073287964,\n",
       "   1.1445205211639404,\n",
       "   0.7563074231147766,\n",
       "   1.3018412590026855,\n",
       "   0.7184426784515381,\n",
       "   2.6137003898620605,\n",
       "   1.6721456050872803,\n",
       "   0.7669103145599365,\n",
       "   1.3946709632873535,\n",
       "   -0.23801441490650177,\n",
       "   -0.1386905014514923,\n",
       "   -1.7706565856933594,\n",
       "   1.0188120603561401,\n",
       "   -0.5218936204910278,\n",
       "   -1.0881929397583008,\n",
       "   0.003657005727291107,\n",
       "   -0.9681256413459778,\n",
       "   0.5236650705337524,\n",
       "   0.38910186290740967,\n",
       "   1.0188301801681519,\n",
       "   1.6160932779312134,\n",
       "   0.42412441968917847,\n",
       "   1.8023470640182495,\n",
       "   -0.11842626333236694,\n",
       "   2.722759246826172,\n",
       "   0.0128400269895792,\n",
       "   0.025150954723358154,\n",
       "   0.7900563478469849,\n",
       "   -0.9426090121269226,\n",
       "   0.6924219131469727,\n",
       "   -0.7195737361907959,\n",
       "   0.7509828209877014,\n",
       "   -1.599413514137268,\n",
       "   0.8491089940071106,\n",
       "   1.0113667249679565,\n",
       "   -0.9890730977058411,\n",
       "   0.3407444357872009,\n",
       "   0.8343977332115173,\n",
       "   -0.3360089361667633,\n",
       "   -0.5986436605453491,\n",
       "   0.6213425397872925,\n",
       "   -1.1347668170928955,\n",
       "   -0.5658785700798035,\n",
       "   0.2026015818119049,\n",
       "   -1.0367741584777832,\n",
       "   1.772717833518982,\n",
       "   0.20908096432685852,\n",
       "   -0.8564481735229492,\n",
       "   -0.520882248878479,\n",
       "   -0.185764878988266,\n",
       "   -0.3893870413303375,\n",
       "   0.3040013909339905,\n",
       "   -0.0579773373901844,\n",
       "   0.15761156380176544,\n",
       "   0.956800103187561,\n",
       "   0.8376765847206116,\n",
       "   1.294787883758545,\n",
       "   -0.3658280372619629,\n",
       "   1.2706974744796753,\n",
       "   -0.0831918716430664,\n",
       "   0.6038703918457031,\n",
       "   -0.483521044254303,\n",
       "   0.44407108426094055,\n",
       "   0.6737134456634521,\n",
       "   0.19087809324264526,\n",
       "   0.8423948884010315,\n",
       "   0.48666682839393616,\n",
       "   0.42637720704078674,\n",
       "   0.21357399225234985,\n",
       "   1.5941460132598877,\n",
       "   1.963003396987915,\n",
       "   -0.19069817662239075,\n",
       "   0.9123062491416931,\n",
       "   1.1084014177322388,\n",
       "   -0.5849800705909729,\n",
       "   0.6616327166557312,\n",
       "   0.060416728258132935,\n",
       "   -0.8444586396217346,\n",
       "   -0.20163552463054657,\n",
       "   1.360659122467041,\n",
       "   1.4116050004959106,\n",
       "   -2.331338882446289,\n",
       "   0.2591554522514343,\n",
       "   0.35619616508483887,\n",
       "   -0.32250547409057617,\n",
       "   1.7371025085449219,\n",
       "   -0.33418431878089905,\n",
       "   -1.1154329776763916,\n",
       "   -0.299721360206604,\n",
       "   1.3681185245513916,\n",
       "   -0.5048493146896362,\n",
       "   0.5808183550834656,\n",
       "   0.24945223331451416,\n",
       "   0.37080445885658264,\n",
       "   -0.001870749518275261,\n",
       "   -0.07948103547096252,\n",
       "   0.4772772789001465,\n",
       "   0.40643030405044556,\n",
       "   1.184998869895935,\n",
       "   -0.5573731660842896,\n",
       "   0.11501304805278778,\n",
       "   0.22162450850009918,\n",
       "   1.766296148300171,\n",
       "   -0.058019015938043594,\n",
       "   0.23768900334835052,\n",
       "   -0.41587817668914795],\n",
       "  'facial_area': {'x': 202,\n",
       "   'y': 155,\n",
       "   'w': 490,\n",
       "   'h': 490,\n",
       "   'left_eye': (518, 339),\n",
       "   'right_eye': (357, 353)},\n",
       "  'face_confidence': 0.85}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = r'C:\\Users\\User\\Pictures\\juan_esguerra.png'\n",
    "\n",
    "repre(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mDeepFace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepresent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mimg_path\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmodel_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'VGG-Face'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0menforce_detection\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdetector_backend\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'opencv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0malign\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mexpand_percentage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mnormalization\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'base'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0manti_spoofing\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_faces\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Represent facial images as multi-dimensional vector embeddings.\n",
      "\n",
      "Args:\n",
      "    img_path (str or np.ndarray): The exact path to the image, a numpy array in BGR format,\n",
      "        or a base64 encoded image. If the source image contains multiple faces, the result will\n",
      "        include information for each detected face.\n",
      "\n",
      "    model_name (str): Model for face recognition. Options: VGG-Face, Facenet, Facenet512,\n",
      "        OpenFace, DeepFace, DeepID, Dlib, ArcFace, SFace and GhostFaceNet\n",
      "        (default is VGG-Face.).\n",
      "\n",
      "    enforce_detection (boolean): If no face is detected in an image, raise an exception.\n",
      "        Default is True. Set to False to avoid the exception for low-resolution images\n",
      "        (default is True).\n",
      "\n",
      "    detector_backend (string): face detector backend. Options: 'opencv', 'retinaface',\n",
      "        'mtcnn', 'ssd', 'dlib', 'mediapipe', 'yolov8', 'centerface' or 'skip'\n",
      "        (default is opencv).\n",
      "\n",
      "    align (boolean): Perform alignment based on the eye positions (default is True).\n",
      "\n",
      "    expand_percentage (int): expand detected facial area with a percentage (default is 0).\n",
      "\n",
      "    normalization (string): Normalize the input image before feeding it to the model.\n",
      "        Default is base. Options: base, raw, Facenet, Facenet2018, VGGFace, VGGFace2, ArcFace\n",
      "        (default is base).\n",
      "\n",
      "    anti_spoofing (boolean): Flag to enable anti spoofing (default is False).\n",
      "\n",
      "    max_faces (int): Set a limit on the number of faces to be processed (default is None).\n",
      "\n",
      "Returns:\n",
      "    results (List[Dict[str, Any]]): A list of dictionaries, each containing the\n",
      "        following fields:\n",
      "\n",
      "    - embedding (List[float]): Multidimensional vector representing facial features.\n",
      "        The number of dimensions varies based on the reference model\n",
      "        (e.g., FaceNet returns 128 dimensions, VGG-Face returns 4096 dimensions).\n",
      "\n",
      "    - facial_area (dict): Detected facial area by face detection in dictionary format.\n",
      "        Contains 'x' and 'y' as the left-corner point, and 'w' and 'h'\n",
      "        as the width and height. If `detector_backend` is set to 'skip', it represents\n",
      "        the full image area and is nonsensical.\n",
      "\n",
      "    - face_confidence (float): Confidence score of face detection. If `detector_backend` is set\n",
      "        to 'skip', the confidence will be 0 and is nonsensical.\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\deepface\\deepface.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "DeepFace.represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture the screen feed from the second monitor.\n",
    "with mss() as sct:\n",
    "    monitor = sct.monitors[2]\n",
    "    while True:\n",
    "        # Grab the screen image.\n",
    "        img = np.array(sct.grab(monitor))\n",
    "        # Convert image from BGRA to BGR.\n",
    "        screen_frame = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "        \n",
    "        # Detect faces and perform recognition.\n",
    "        processed_frame = detect_and_recognize(screen_frame)\n",
    "        \n",
    "        cv2.imshow(\"My Face Detection & Recognition Project\", processed_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition error: Face could not be detected in numpy array.Please confirm that the picture is a face photo or consider to set enforce_detection param to False.\n",
      "Recognition error: Face could not be detected in numpy array.Please confirm that the picture is a face photo or consider to set enforce_detection param to False.\n"
     ]
    }
   ],
   "source": [
    "### testing with an video stream from my second monitor\n",
    "### and attendance insertion table. \n",
    "\n",
    "\n",
    "\n",
    "# Load Haar cascade for face detection.\n",
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "\n",
    "# Optionally, precompute your reference embeddings.\n",
    "# For example, you might have a folder \"reference_faces\" with subfolders or images named with the label.\n",
    "# You can run this once at startup to build your embeddings database.\n",
    "# Alternatively, DeepFace.find() can search a folder directly.\n",
    "\n",
    "\n",
    "##db_path = r\"C:\\Users\\User\\Pictures\\Face_recog_db\"\n",
    "\n",
    "def detect_and_recognize(screen_frame):\n",
    "    # Convert frame to grayscale for face detection.\n",
    "    gray_image = cv2.cvtColor(screen_frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))\n",
    "\n",
    "    datest =date.today()\n",
    "    formatted_date = datest.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract face region of interest (ROI)\n",
    "        face_roi = screen_frame[y:y+h, x:x+w]\n",
    "        # Convert ROI from BGR to RGB (DeepFace expects RGB images)\n",
    "        face_roi_rgb = face_roi ##cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        try:\n",
    "            # Use DeepFace.find() to compare the detected face against your reference database.\n",
    "            # Setting enforce_detection=False to avoid errors if a face isn't perfectly aligned.\n",
    "            \n",
    "            \n",
    "            ##results = DeepFace.find(img_path=face_roi_rgb, db_path=db_path, model_name= \"Facenet\", enforce_detection=False)\n",
    "            results = DeepFace.represent(img_path=face_roi_rgb, model_name=\"Facenet\", anti_spoofing= True)\n",
    "\n",
    "\n",
    "            if len(results) >0 and len(results[0][\"embedding\"]) > 0:\n",
    "\n",
    "                embedding_target = results[0][\"embedding\"] \n",
    "\n",
    "                snow_query_search = f\"\"\"\n",
    "                                        SELECT   IMG_NAME, VECTOR_L2_DISTANCE(EMBEDDING,{embedding_target}::VECTOR(FLOAT,128)) as SIMILARITY\n",
    "                                        FROM RECOG_DB.ATTEND.embeddings_test\n",
    "                                        ORDER BY SIMILARITY ASC\n",
    "                                        LIMIT 1\n",
    "                                    \"\"\"\n",
    "                \n",
    "                \n",
    "                best_match = cs.execute(snow_query_search).fetchall()\n",
    "\n",
    "                label = best_match[0][0].split('/')[-1].split('.')[0]\n",
    "\n",
    "                attendance_merge = f\"\"\"\n",
    "                MERGE INTO RECOG_DB.ATTEND.attendance_test AS target\n",
    "                USING (\n",
    "                    SELECT '{label}' AS student_name,\n",
    "                           'CAPSTONE_PROJECT' AS class_name,\n",
    "                           '{formatted_date}'::DATE AS attendance_date\n",
    "                ) AS source\n",
    "                ON target.student_name = source.student_name \n",
    "                   AND target.attendance_date = source.attendance_date\n",
    "                WHEN NOT MATCHED THEN\n",
    "                    INSERT (student_name, class_name, attendance_date)\n",
    "                    VALUES (source.student_name, source.class_name, source.attendance_date)\n",
    "                \"\"\"\n",
    "                cs.execute(attendance_merge)\n",
    "                \n",
    "            else:\n",
    "                label = 'unknown'\n",
    "                \n",
    "\n",
    "        except Exception as e:\n",
    "            # If DeepFace fails to process, label as Unknown.\n",
    "            label = \"Unknown_from_exception\"\n",
    "            print(\"Recognition error:\", e)\n",
    "        \n",
    "        # Draw rectangle and label on the frame.\n",
    "        cv2.rectangle(screen_frame, (x, y), (x+w, y+h), (0, 255, 0), 4)\n",
    "        cv2.putText(screen_frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    return screen_frame\n",
    "\n",
    "# Capture the screen feed from the second monitor.\n",
    "with mss() as sct:\n",
    "    monitor = sct.monitors[2]\n",
    "    last_sent = 0  # time when the last frame was processed/sent\n",
    "    while True:\n",
    "        # Grab the screen image.\n",
    "        img = np.array(sct.grab(monitor))\n",
    "        # Convert image from BGRA to BGR.\n",
    "        screen_frame = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "        \n",
    "        # Check if 5 seconds have passed since the last processed frame\n",
    "        now = time.time()\n",
    "        if now - last_sent >= 20:\n",
    "            processed_frame = detect_and_recognize(screen_frame)\n",
    "            cv2.imshow(\"My Face Detection & Recognition Project\", processed_frame)\n",
    "            last_sent = now  # update last_sent time\n",
    "\n",
    "        # Even if not sending a frame, check for the 'q' key press to exit.\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-05\n"
     ]
    }
   ],
   "source": [
    "datest =date.today()\n",
    "formatted_date = datest.strftime(\"%Y-%m-%d\")\n",
    "print(formatted_date) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "import snowflake.connector as sc\n",
    "\n",
    "\n",
    "\n",
    "def get_secret():\n",
    "    secret_name = \"snowflake/face_rec/rsa_private_key\"\n",
    "    region_name = \"us-east-2\"\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "\n",
    "    secret = get_secret_value_response['SecretString']\n",
    "    return secret\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_private_key(pem_str, key_password=None):\n",
    "    \"\"\"\n",
    "    Load a PKCS8 formatted private key from a PEM string.\n",
    "    key_password should be a string if the key is encrypted, otherwise None.\n",
    "    \"\"\"\n",
    "    private_key = serialization.load_pem_private_key(\n",
    "        pem_str.encode(\"utf-8\"),\n",
    "        password=key_password.encode(\"utf-8\") if key_password else None,\n",
    "        backend=default_backend()\n",
    "    )\n",
    "    return private_key\n",
    "\n",
    "# Retrieve the private key string from AWS Secrets Manager.\n",
    "pem_private_key = get_secret()\n",
    "\n",
    "print(type(pem_private_key))\n",
    "\n",
    "\n",
    "##first_value = next(iter(pem_private_key.values()))\n",
    "##print(first_value)\n",
    "\n",
    "# If your key is not encrypted, key_password is None.\n",
    "key_password = None  # Set this if your private key is password-protected\n",
    "\n",
    "\n",
    "\n",
    "##fixed_private_key = fix_pem_format(first_value)\n",
    "\n",
    "# Load the private key into a cryptography object.\n",
    "private_key = load_private_key(pem_private_key, key_password=key_password)\n",
    "\n",
    "##private_key = r\"D:\\Users\\Andrsfch\\Documents\\Snowflake Configs\\rsa_key.p8\"\n",
    "\n",
    "# Snowflake connection parameters using key-pair authentication.\n",
    "conn_params = {\n",
    "    'account': 'TIMCEXC-MYB81917',\n",
    "    'user': 'face_rec_service',\n",
    "    'private_key': private_key,\n",
    "    'warehouse': 'COMPUTE_WH',\n",
    "    'database': 'RECOG_DB',\n",
    "    'schema': 'ATTEND'\n",
    "}\n",
    "\n",
    "# Connect to Snowflake.\n",
    "ctx = sc.connect(**conn_params)\n",
    "cs = ctx.cursor()\n",
    "\n",
    "# Example query.\n",
    "cs.execute(\"SELECT *  FROM RECOG_DB.ATTEND.institution\")\n",
    "print(cs.fetchone())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "\n",
    "def fix_pem_format(pem_str):\n",
    "    \"\"\"\n",
    "    Reformat a PEM string to ensure proper line breaks.\n",
    "    This assumes that the key is in the format:\n",
    "    \"-----BEGIN PRIVATE KEY----- <base64 data> -----END PRIVATE KEY-----\"\n",
    "    and rewraps the base64 data at 64-character intervals.\n",
    "    \"\"\"\n",
    "    header = \"-----BEGIN PRIVATE KEY-----\"\n",
    "    footer = \"-----END PRIVATE KEY-----\"\n",
    "    \n",
    "    # Remove any extra whitespace at the ends.\n",
    "    pem_str = pem_str.strip()\n",
    "    \n",
    "    if header in pem_str and footer in pem_str:\n",
    "        # Split by header and footer\n",
    "        start = pem_str.find(header) + len(header)\n",
    "        end = pem_str.find(footer)\n",
    "        # Remove spaces from the middle section\n",
    "        base64_data = pem_str[start:end].strip().replace(\" \", \"\")\n",
    "        # Rewrap the base64 content to 64-character lines\n",
    "        wrapped = \"\\n\".join(textwrap.wrap(base64_data, 64))\n",
    "        fixed_pem = f\"{header}\\n{wrapped}\\n{footer}\"\n",
    "        return fixed_pem\n",
    "    else:\n",
    "        # If header/footer not found, return as-is\n",
    "        return pem_str\n",
    "\n",
    "# Assume pem_private_key_str is the key string retrieved from Secrets Manager.\n",
    "pem_private_key_str = \"\"\"\n",
    "-----BEGIN PRIVATE KEY-----\n",
    "MIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDdb/MYUexc/8su\n",
    "5Oak6Intul91OxsWz4EDQ59MTNaTi0cdMLcX07qNKyylK8J3jze9Sqzz1qfiprv4\n",
    "ZxeGahFQV0mdB8uKuklA7Sit5H9yLAxU/gCmsWf1Y6bPF3IK/+x57sws5YwOS2ME\n",
    "i9ofRUxP357jWHPVGUmMHL54s4HKZE8dF1Vp7xdtDXSErcH0pht1N4OvPQgP+fLL\n",
    "OruEJWtSvUUsyf9Bp+RZd2sH80aQNFrkhsBMCqibIqXv1uohkM7+pG+eHrBzw6W2\n",
    "jSikuF0TeSFao+6mAXPZ1bZtzeSDjfxumvu+W7tzhEHmCZ6FhDGxT2bN8BwwQr9H\n",
    "Vb+zslsNAgMBAAECggEAK+kD693s8LkhjaRC708UcNF9FeZZVhHiS6coas0u3LWo\n",
    "7asoF3+7FbzKvtsZVjOOckkhWD4H7xwwmI663cLhOY0BkjPGQzVzCr6siyUNN7Lv\n",
    "5Be1QdjanFC9NoI38WHz/u7hSoLfXcNJuJBR+dL4UbtuircSyx5XVQMrKwyvj1Yr\n",
    "saj3b9k9MG9uDK51XURlT80WmfEq+lSRaCiuBQKReuC4nlky5yxmib8buU/u+G7I\n",
    "9o1oGs43fEMQnzTiLyGzEoItTxPaQHQYSvqFMHJVauLUxfbvKsTqTXHNuI9xJBAT\n",
    "PIDCrJYTxC8Q6DgG5gBtImk2ujDjJrjKl+Zl0Sx/gQKBgQD96APiOgT0lWMmsVdY\n",
    "PhXiBup2bKgRWKvnN95K46+NtztdZxcCOuf0DQ8TZLqW439GLdyYE5a36xPkzoUJ\n",
    "nC7oQPvhOPYRohOtvuh+yXd5uD4D1GbtSnzmcWnVBtL8xk/R8XbDWt6YcgNUtfID\n",
    "6LX0FrLwtoRUTsN1JSS1V589gQKBgQDfQ2TQT8xo8mTkDku18BuvW8Q03mYK+y0O\n",
    "E+xbAPFyKWTAVD4vQ/89yRfFJj/do64nZ+qkjgmhUmdkJtIS3ufajZasYaix9dKk\n",
    "ts21HCpSKZSLZKrkZXWH2caYScI7nw9iqcdmtVUIcmK7QXpHEIyP9TBi5Z2K/WgU\n",
    "CDgMh7D7jQKBgHkd2exqPL7nG9rzMXUYeJvPLLy87BcKM0YicllK7w0s4JaXfHRF\n",
    "4SubpxtbK16aurWhXt70SSsGagD1OqTw6bdEeGJhJ5AVvkldDc6dD+7yIjL1ePus\n",
    "UpQieZdyn+aQOrcs9bgOeqxBhrE78WxH77O1uDn29ny47afNxgry9zUBAoGBAMTz\n",
    "Obq9uk+WDMRVjBpaTsHZzPLqgdAMFYnGr4j0Awo3U8aKSjT94erKO2rSPCK0Ioxm\n",
    "kkpJdltWSxjBWi4FbziQEKvOlrcuaHyfOkYbK4ZBYeW5jztd+Vk3Fob/AhYAUd0y\n",
    "clG7guCpr0kLEOghxC22GNIVRm3271pEidUb2ld9AoGBAIx8dL6eumLy4a34h3/t\n",
    "ic1Gy6FhHkBwBBSSMqoNKqae+3fASNBaT+z+ZCoZQ6Nf+ORw+Fpeeu8K1NB3+U39\n",
    "hrgyvhyWPnOED4hs5bgJEYU+AZFJ1QRN7JxzkjMWJps+XYmNLqm0xWIOLDSs+2jN\n",
    "S7us6arqgRM76oXJ1paLsmhW\n",
    "-----END PRIVATE KEY-----\n",
    "\n",
    "\"\"\"\n",
    "# Fix the PEM formatting:\n",
    "fixed_pem = fix_pem_format(pem_private_key_str)\n",
    "print(repr(fixed_pem))\n",
    "\n",
    "# Now try loading the key:\n",
    "try:\n",
    "    private_key = serialization.load_pem_private_key(\n",
    "        fixed_pem.encode(\"utf-8\"),\n",
    "        password=None,  # or provide your key password if needed\n",
    "        backend=default_backend()\n",
    "    )\n",
    "    print(\"Private key loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to load private key:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a session with your AWS access keys and region\n",
    "session = boto3.session.Session(\n",
    "    aws_access_key_id='AKIATG6MGZ27MEZI7EM3',\n",
    "    aws_secret_access_key='NjwfpnIGoKqZYNneFLd6MxRc9QMc15w8YXi5jD3a',\n",
    "    region_name='us-east-2'  # Change to your desired region\n",
    ")\n",
    "\n",
    "# Create a client for Secrets Manager (or any other service)\n",
    "client = session.client('secretsmanager')\n",
    "\n",
    "# Example: List secrets to verify the connection\n",
    "response = client.list_secrets()\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
